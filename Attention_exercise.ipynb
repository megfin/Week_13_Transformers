{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Attention_exercise.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxVVWTIu9VSt",
        "colab_type": "text"
      },
      "source": [
        "# Week 13 Exercise - Transformers\n",
        "\n",
        "In this notebook, we will explore the creation of a Transformer Network for English to French translation.  Note that **Transformers are resource intensive and hard to train.** You will want to run these notebooks on a machine equipped with a GPU or on [Google Colab](http://colab.research.google.com).\n",
        "\n",
        "To begin, let's import a corpus of paired English and French text.  Additionally, we'll tokenize the words (i.e. create a dictionary for each vocabulary associating every word with an integer index).  There is no need to modify this cell, but have a look at what is contained in fr_to_ix (for example) and in enlines.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BaNa-FkAsW2",
        "colab_type": "code",
        "outputId": "a0a189e8-8c24-42d6-951c-f0696e4e387a",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-45c325fc-a244-413c-bf35-75760bf4485b\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-45c325fc-a244-413c-bf35-75760bf4485b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving english.txt to english (1).txt\n",
            "Saving french.txt to french (1).txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTeQSGsU9VSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if torch.cuda.is_available():  \n",
        "    device = \"cuda:0\" \n",
        "else:  \n",
        "    device = \"cpu\" \n",
        "\n",
        "with open('./french.txt', encoding=\"utf-8\") as file:\n",
        "    frvocab = file.read().lower()\n",
        "    frvocab = ''.join([i if ord(i) < 128 else ' ' for i in frvocab])\n",
        "    frlines = frvocab.split('\\n')\n",
        "frlines = [re.sub(r'[^\\w\\s\\']','',i).split() for i in frlines]\n",
        "frvocab = set(re.sub(r'[^\\w\\s\\']','',frvocab).replace('\\n',' ').split(' '))\n",
        "\n",
        "with open('./english.txt', encoding=\"utf-8\") as file:\n",
        "    envocab = file.read().lower()\n",
        "    envocab = ''.join([i if ord(i) < 128 else '' for i in envocab])\n",
        "    enlines = envocab.split('\\n')\n",
        "enlines = [re.sub(r'[^\\w\\s]','',i).split() for i in enlines]\n",
        "envocab = set(re.sub(r'[^\\w\\s]','',envocab).replace('\\n',' ').strip().split(' '))\n",
        "envocab.add('<pad>')\n",
        "envocab.add('<start>')\n",
        "envocab.add('<eos>')\n",
        "frvocab.add('<pad>')\n",
        "frvocab.add('<start>')\n",
        "frvocab.add('<eos>')\n",
        "fr_to_ix = {word: i for i, word in enumerate(frvocab)}\n",
        "en_to_ix = {word: i for i, word in enumerate(envocab)}\n",
        "ix_to_fr = {fr_to_ix[word]:word for word in frvocab}\n",
        "ix_to_en = {en_to_ix[word]:word for word in envocab}\n",
        "enmax = 0\n",
        "frmax = 16\n",
        "\n",
        "for i,w in enumerate(enlines):\n",
        "    temp = len(w)\n",
        "    if temp > enmax:\n",
        "        enmax = temp\n",
        "\n",
        "for i,w in enumerate(frlines):\n",
        "    temp = len(w)\n",
        "    if temp > frmax:\n",
        "        frmax = temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w685K9T9VS1",
        "colab_type": "text"
      },
      "source": [
        "Next we'll create a handful of helper functions that do things like\n",
        " - Tokenize an english string, run it through the transformer producing predictions, then convert back to a french string\n",
        " - Compare predicted and target output\n",
        " - Mask a string\n",
        " - Load paired english/french sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWrVRTLR9VS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    # Read in an english string\n",
        "    line = re.sub(r'[^\\w\\s]','',sentence).split()\n",
        "    # tokenize/pad for consistent sequence length\n",
        "    line = F.pad(torch.tensor([en_to_ix[w.lower()] for w in line]),(0,enmax-len(line)),value = en_to_ix['<pad>']).unsqueeze(0).to(device)\n",
        "    # Create an array to hold the French sentence\n",
        "    target = torch.Tensor(1,frmax-1)\n",
        "    target = target.new_full((1,frmax-1),fr_to_ix['<pad>']).long().to(device)\n",
        "    # Start sentence with a <start> character\n",
        "    target[0,0] = fr_to_ix['<start>']\n",
        "    \n",
        "    src,trg = mask(line,target)\n",
        "    encoding = model.encode(line,src)\n",
        "    K,V = model.create_dec_KV(encoding)\n",
        "    for i in range(1,frmax-1):\n",
        "        test2 = model.decode(target,K,V,src,trg)\n",
        "        lastout = test2[0,i-1].argmax()\n",
        "        if lastout.item() == fr_to_ix['<eos>']:\n",
        "            break\n",
        "        target[0,i] = lastout\n",
        "        src,trg = mask(line,target)\n",
        "    translation = test2.argmax(2).squeeze(0)\n",
        "    translation_string = ''\n",
        "    for w in translation:\n",
        "        if ix_to_fr[w.item()] == '<eos>':\n",
        "            break\n",
        "        translation_string += ix_to_fr[w.item()] + ' '\n",
        "    return translation_string.strip()\n",
        "\n",
        "def compareoutput(preds,targetlist,loc=None):\n",
        "    # Compare model predictions with true translation\n",
        "    if loc is None:\n",
        "        loc = np.random.randint(len(preds))\n",
        "    predstr = ''\n",
        "    labelstr = ''\n",
        "    for i in range(len(preds[loc][0])):\n",
        "        if ix_to_fr[targetlist[loc][i+1].item()] == '<eos>':\n",
        "            break\n",
        "        predstr += ' '+ ix_to_fr[preds[loc][0][i].item()]\n",
        "        labelstr += ' ' + ix_to_fr[targetlist[loc][i+1].item()]\n",
        "    print(\"\\tOutput:\", predstr)\n",
        "    print(\"\\tTarget:\",labelstr)\n",
        "    \n",
        "class PositionalEncoder(nn.Module):\n",
        "    # Create a positional encoding generator\n",
        "    def __init__(self, d_model, max_seq_len = 58):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # create constant 'pe' matrix with values dependant on \n",
        "        # pos and i\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = \\\n",
        "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "            for i in range(1,d_model,2):\n",
        "                pe[pos, i] = \\\n",
        "                math.cos(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                \n",
        "        pe = pe\n",
        "        self.register_buffer('pe', pe)\n",
        " \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # make embeddings relatively larger\n",
        "        x = x * math.sqrt(self.d_model)\n",
        "        #add constant to embedding\n",
        "        seq_len = x.size(1)\n",
        "        x = x + Variable(self.pe[:seq_len], \\\n",
        "        requires_grad=False).to(device)\n",
        "        return x\n",
        "\n",
        "def mask(input_seq,target_seq):\n",
        "    input_msk = (input_seq != en_to_ix['<pad>']).unsqueeze(1)\n",
        "    target_msk = (target_seq != fr_to_ix['<pad>']).unsqueeze(1)\n",
        "    size = target_seq.size(1) # get seq_len for matrix\n",
        "    nopeak_mask = np.triu(np.ones((1, size, size)),k=1)\n",
        "    nopeak_mask = Variable(torch.from_numpy(nopeak_mask).to(device) == 0)\n",
        "    target_msk = target_msk & nopeak_mask\n",
        "    return input_msk,target_msk\n",
        "\n",
        "class custdata(Dataset):\n",
        "    # Create a custom dataset object to serve up paired english and french lines\n",
        "    def __init__(self,enlines,frlines):\n",
        "        self.data_len = len(enlines) \n",
        "        self.data = [F.pad(torch.tensor([en_to_ix[w] for w in line]),(0,enmax-len(line)),value = en_to_ix['<pad>']).to(device) for line in enlines]\n",
        "        self.labels = []\n",
        "        for line in frlines:\n",
        "            line = ['<start>',*line,'<eos>']\n",
        "            self.labels.append(F.pad(torch.tensor([fr_to_ix[w] for w in line]),(0,frmax-len(line)),value = fr_to_ix['<pad>']).to(device))\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        return self.data[i],self.labels[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLTDsRx19VS8",
        "colab_type": "text"
      },
      "source": [
        "# Attention\n",
        "The first task is to code a self-attention mechanism, which corresponds to implementing Eq. 1 in Vaswani.\n",
        "#### http://jalammar.github.io/illustrated-transformer/ is a great reference for most of the programming in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxDtWjV99VS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class self_attention(nn.Module):\n",
        "    def __init__(self,dim,enc_dim,dropout = .1):\n",
        "        super().__init__()\n",
        "        self.wq = nn.Linear(dim,enc_dim) #### TODO#### WEIGHTS FOR Q, INPUT DIM = DIM, OUTPUT DIM = ENC_DIM\n",
        "        self.wk = nn.Linear(dim,enc_dim) #### TODO#### WEIGHTS FOR K, INPUT DIM = DIM, OUTPUT DIM = ENC_DIM\n",
        "        self.wv = nn.Linear(dim,enc_dim) #### TODO#### WEIGHTS FOR V, INPUT DIM = DIM, OUTPUT DIM = ENC_DIM\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scaler = np.sqrt(enc_dim)\n",
        "    \n",
        "    def QKV(self,x):\n",
        "        Q = self.wq(x)  #### TODO#### CALCULATE Q\n",
        "        K = self.wk(x)  #### TODO#### CALCULATE K\n",
        "        V = self.wv(x)  #### TODO#### CALCULATE V\n",
        "        return Q,K,V\n",
        "    \n",
        "    def score(self,Q,K,V,mask):\n",
        "        # scores are the stuff that goes inside the softmax\n",
        "        scores = Q@K.T.permute(2,0,1)/self.scaler ### TODO ### CALCULATE THE SCORES. !!!DONT TOUCH THE PERMUTE!!!\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        scores = self.dropout(F.softmax(scores,-1)) \n",
        "        return scores@V ### TODO ### FINISH CALCULATING SELF ATTENTION\n",
        "    \n",
        "    def forward(self,x,mask=None):\n",
        "        Q,K,V = self.QKV(x)\n",
        "        return self.score(Q,K,V,mask)\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JjYnv1K9VTC",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to produce the \"special\" attention mechanism that takes keys and values from the encoder, but queries from the decoder.  This is very similar to the self-attention mechanism, except that there should be two inputs.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRW7-nu_9VTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class encdec_attention(nn.Module):\n",
        "    def __init__(self,dim,dropout = .1):\n",
        "        super().__init__()\n",
        "        self.wq = nn.Linear(dim,dim) #### TODO #### SAME AS ABOVE\n",
        "        self.wk = nn.Linear(dim,dim) #### TODO #### SAME AS ABOVE\n",
        "        self.wv = nn.Linear(dim,dim) #### TODO #### SAME AS ABOVE\n",
        "        self.scaler = np.sqrt(dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def Q(self,x):\n",
        "        return self.wq(x)\n",
        "    \n",
        "    def score(self,Q,K,V,mask):\n",
        "        scores = Q@K.T.permute(2,0,1)/self.scaler #### TODO #### SAME AS ABOVE\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        scores = self.dropout(F.softmax(scores,-1)) \n",
        "        return scores@V  #### TODO #### SAME AS ABOVE\n",
        "    \n",
        "    def forward(self,x,K,V,mask):\n",
        "        # DB Note: I'm not sure that this signature is right.  Seems like we should be taking x from the\n",
        "        # decoder, as well as another argument (call it y?) from the encoder, then producing K,V,Q internally,\n",
        "        # just like in the self-attention scheme.  Otherwise, how are wk and wv being used here?  it looks like \n",
        "        # these parameters have been shifted over to the Transformer module's create_dec_KV method.\n",
        "        Q = self.Q(x)\n",
        "        out = self.score(Q,K,V,mask)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHGVTCEj9VTI",
        "colab_type": "text"
      },
      "source": [
        "# Encoder and Decoder\n",
        "With the attention mechanisms coded, now we need to create encoder and decoder models. These correspond to the things inside the boxes in Figure 1 of Vaswani.  \n",
        "#### Fill in the forward passes of the encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHHEOOdZ9VTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class encoder(nn.Module):\n",
        "    def __init__(self,dim,enc_dim,vocab_size,dropout=.1):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.enc_dim = enc_dim\n",
        "        self.residual = nn.Linear(dim,enc_dim)\n",
        "        \n",
        "        self.attention = self_attention(dim,enc_dim,dropout)\n",
        "        self.norm1 = nn.LayerNorm(enc_dim)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(enc_dim,enc_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(enc_dim)\n",
        "    \n",
        "    def forward(self,x,mask):  #### TODO #### SET UP FORWARD PASS OF ENCODER\n",
        "        attention_out = self.attention(x,mask)\n",
        "\n",
        "        if self.dim != self.enc_dim: ### DONT TOUCH, THIS IS TO HELP WITH THE RESIDUAL CONNECTION ###\n",
        "            x = self.residual(x)\n",
        "        norm1_out = self.norm1(x+attention_out)\n",
        "        linear_out = self.linear(norm1_out)\n",
        "        norm2_out = self.norm2(linear_out+norm1_out)        \n",
        "        return norm2_out\n",
        "     \n",
        "        \n",
        "class decoder(nn.Module):\n",
        "    def __init__(self,dim,input_size,vocab_size,dropout=.1):\n",
        "        super().__init__()\n",
        "        self.attention = self_attention(input_size,dim,dropout)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.EDattention = encdec_attention(dim,dropout)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(dim,dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.norm3 = nn.LayerNorm(dim)\n",
        "    \n",
        "    def forward(self,x,k,v,enc_mask,dec_mask):#### TODO #### SET UP FORWARD PASS OF DECODER\n",
        "        attention_out = self.attention(x,dec_mask)\n",
        "        norm1_out = self.norm1(x+attention_out)\n",
        "        enc_dec_attention = self.EDattention(norm1_out,k,v,enc_mask)\n",
        "        norm2_out = self.norm2(norm1_out+enc_dec_attention)\n",
        "        linear_out = self.linear(norm2_out)\n",
        "        norm3_out = self.norm3(norm2_out+linear_out)\n",
        "        return norm3_out\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXeRaYps9VTO",
        "colab_type": "text"
      },
      "source": [
        "# Transformer\n",
        "\n",
        "Build the transformer itself by hooking together encoders and decoders.  Note the word embedding layers that we are going to learn.  \n",
        "\n",
        "#### Add encoders and decoders to transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1nh9sRa9VTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class transformer(nn.Module):\n",
        "    def __init__(self,dim,encoder_dim,enc_vocab_size,dec_vocab_size,input_size):\n",
        "        super().__init__()\n",
        "        self.embedding1 = nn.Embedding(enc_vocab_size,dim)\n",
        "        self.embedding2 = nn.Embedding(dec_vocab_size,encoder_dim)\n",
        "        \n",
        "        self.pe1 = PositionalEncoder(dim,enmax)\n",
        "        self.pe2 = PositionalEncoder(encoder_dim,frmax)\n",
        "        self.encoders = []\n",
        "    \n",
        "        self.encoders.append(encoder(dim,encoder_dim,enc_vocab_size)) #### TODO #### ADD DESIRED # OF ENCODERS TO SELF.ENCODERS\n",
        "        self.encoders.append(encoder(dim,encoder_dim,enc_vocab_size))\n",
        "        self.encoders.append(encoder(dim,encoder_dim,enc_vocab_size))\n",
        "        \n",
        "        self.encoders = nn.ModuleList(self.encoders)\n",
        "        \n",
        "        self.decoders = []\n",
        "        \n",
        "        self.decoders.append(decoder(encoder_dim,encoder_dim,dec_vocab_size)) #### TODO #### ADD DESIRED # OF DECODERS TO SELF.DECODERS\n",
        "        self.decoders.append(decoder(encoder_dim,encoder_dim,dec_vocab_size))\n",
        "        self.decoders.append(decoder(encoder_dim,encoder_dim,dec_vocab_size))\n",
        "        \n",
        "\n",
        "       \n",
        "        self.decoders = nn.ModuleList(self.decoders)\n",
        "        \n",
        "        self.final = nn.Sequential(\n",
        "            nn.Linear(encoder_dim,dec_vocab_size),\n",
        "            nn.LogSoftmax(2)\n",
        "        )\n",
        "        \n",
        "        self.k = nn.Linear(encoder_dim,encoder_dim)\n",
        "        self.v = nn.Linear(encoder_dim,encoder_dim)\n",
        "    def create_dec_KV(self,z):\n",
        "        K = self.k(z)\n",
        "        V = self.v(z)\n",
        "        return K,V\n",
        "    \n",
        "    def encode(self,x,src):\n",
        "        x = self.embedding1(x)\n",
        "        x = self.pe1(x)\n",
        "        for layer in self.encoders:\n",
        "            x = layer(x,src)\n",
        "        return x\n",
        "    \n",
        "    def decode(self,y,K,V,src,trg):\n",
        "        y = self.embedding2(y)\n",
        "        y = self.pe2(y)\n",
        "        for layer in self.decoders:\n",
        "            y = layer(y,K,V,src,trg)\n",
        "        return self.final(y)\n",
        "    \n",
        "    def forward(self,x,y,src,trg):\n",
        "        \n",
        "        x = self.encode(x,src)\n",
        "        K,V = self.create_dec_KV(x)\n",
        "        y = self.decode(y,K,V,src,trg)\n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQt4qzkr9VTU",
        "colab_type": "text"
      },
      "source": [
        "# Train the network.\n",
        "\n",
        "##### This will be slow to train and require a lot of resources. You can reduce the batch_size to lower the vram requirement, you can reduce \n",
        "##### the run time by lowering number_of_lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGpUzttR9VTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = transformer(enmax,enmax,len(envocab),len(frvocab),frmax)\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz-s9Gyz9VTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.01,weight_decay=.00001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=.1,patience=10,threshold=1,min_lr=.0001)\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS-OByV09VTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "NUMBER_OF_LINES = 20000\n",
        "\n",
        "train = custdata(enlines[:NUMBER_OF_LINES],frlines[:NUMBER_OF_LINES])\n",
        "trainloader = torch.utils.data.DataLoader(dataset=train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val = custdata(enlines[NUMBER_OF_LINES:NUMBER_OF_LINES+1000],frlines[NUMBER_OF_LINES:NUMBER_OF_LINES+1000])\n",
        "valloader = torch.utils.data.DataLoader(dataset=val, batch_size=1, shuffle=True, drop_last=False)\n",
        "test = custdata(enlines[NUMBER_OF_LINES+1000:NUMBER_OF_LINES+2000],frlines[NUMBER_OF_LINES+1000:NUMBER_OF_LINES+2000])\n",
        "testloader = torch.utils.data.DataLoader(dataset=test, batch_size=1, shuffle=False, drop_last=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az0gSoUY9VTi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b8df240-4547-4434-d6f5-a594987d6548"
      },
      "source": [
        "for i in range(50):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for j,(context, target) in enumerate(trainloader):\n",
        "        trg_input = target[:,:-1]\n",
        "        outmask = target[:,1:]!= fr_to_ix['<pad>']\n",
        "        targets = target[:,1:].contiguous().view(-1)\n",
        "        src,trg = mask(context,trg_input)\n",
        "        output = model(context,trg_input,src,trg)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output.view(output.shape[0]*output.shape[1],-1),targets)\n",
        "        loss.backward()\n",
        "        total_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    scheduler.step(total_loss)\n",
        "    print('Epoch:', i+1,' loss:', total_loss)\n",
        "    model.eval()\n",
        "    scores = []\n",
        "    preds = []\n",
        "    targetlist = []\n",
        "    for j,(context, target) in enumerate(valloader):\n",
        "            trg_input = target[:,:-1]\n",
        "            targets = target.contiguous().view(-1)\n",
        "            targetlist.append(targets)\n",
        "            src,trg = mask(context,trg_input)\n",
        "            output = model(context,trg_input,src,trg)\n",
        "            pred = F.softmax(output,2).argmax(2)\n",
        "            preds.append(pred)\n",
        "            break\n",
        "    compareoutput(preds,targetlist,loc=0)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  loss: 162.1059198975563\n",
            "\tOutput:  <pad> <pad> <pad> <pad> <pad>\n",
            "\tTarget:  vous en avez eu assez\n",
            "Epoch: 2  loss: 100.44587281346321\n",
            "\tOutput:  <eos> <eos> <eos> <eos>\n",
            "\tTarget:  ne perds pas espoir\n",
            "Epoch: 3  loss: 82.3482056260109\n",
            "\tOutput:  <eos> <eos> <eos> <eos>\n",
            "\tTarget:  vous devriez partir maintenant\n",
            "Epoch: 4  loss: 79.29375296831131\n",
            "\tOutput:  <eos> <eos> <eos> <eos>\n",
            "\tTarget:  tu n'es pas normal\n",
            "Epoch: 5  loss: 77.8614190518856\n",
            "\tOutput:  <eos> <eos> <eos> <eos>\n",
            "\tTarget:  touche pas ma bagnole\n",
            "Epoch: 6  loss: 77.56194585561752\n",
            "\tOutput:  <eos> <eos> <eos> <eos>\n",
            "\tTarget:  tesvous int ress s\n",
            "Epoch: 7  loss: 77.53369715809822\n",
            "\tOutput:  <eos> <eos> <eos>\n",
            "\tTarget:  t'as t super\n",
            "Epoch: 8  loss: 74.61246436834335\n",
            "\tOutput:  <eos> <eos> <eos> <eos> <eos>\n",
            "\tTarget:  c'est toi l'a n e\n",
            "Epoch: 9  loss: 72.64003956317902\n",
            "\tOutput:  <eos> <eos> <eos> <eos> <eos>\n",
            "\tTarget:  ne marche pas si vite\n",
            "Epoch: 10  loss: 69.96396446228027\n",
            "\tOutput:  tom <eos> <eos> <eos>\n",
            "\tTarget:  le changement est possible\n",
            "Epoch: 11  loss: 82.15345811843872\n",
            "\tOutput:  <pad> <pad> <pad>\n",
            "\tTarget:  vous tes importantes\n",
            "Epoch: 12  loss: 124.7315303683281\n",
            "\tOutput:  <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "\tTarget:  c'est toi qui as loup une tache\n",
            "Epoch: 13  loss: 114.7608328461647\n",
            "\tOutput:  <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "\tTarget:  tu dois te d p cher\n",
            "Epoch: 14  loss: 112.74956160783768\n",
            "\tOutput:  <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "\tTarget:  vous appr ciez tout le monde\n",
            "Epoch: 15  loss: 111.56774181127548\n",
            "\tOutput:  <pad> <pad> <pad> <pad> <pad>\n",
            "\tTarget:  pourquoi tom atil t vir\n",
            "Epoch: 16  loss: 110.64201563596725\n",
            "\tOutput:  <pad> <pad> <pad> <pad> <pad>\n",
            "\tTarget:  vous tes d go tante\n",
            "Epoch: 17  loss: 109.68501144647598\n",
            "\tOutput:  <pad> <pad> <pad> <pad> <pad>\n",
            "\tTarget:  vous avez besoin de dormir\n",
            "Epoch: 18  loss: 108.87827181816101\n",
            "\tOutput:  <pad> <pad>\n",
            "\tTarget:  tesvous courageuses\n",
            "Epoch: 19  loss: 108.04986727237701\n",
            "\tOutput:  <pad> <pad> <pad> <pad> <pad>\n",
            "\tTarget:  vous tes tr s marrants\n",
            "Epoch: 20  loss: 106.84382075071335\n",
            "\tOutput:  <pad> <pad> <pad> <pad>\n",
            "\tTarget:  tu dois le retenir\n",
            "Epoch: 21  loss: 80.62292209267616\n",
            "\tOutput:  <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "\tTarget:  aussit t gagn aussit t d pens\n",
            "Epoch: 22  loss: 73.43003806471825\n",
            "\tOutput:  <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "\tTarget:  ne fume pas dans le lit\n",
            "Epoch: 23  loss: 68.65149492025375\n",
            "\tOutput:  je je je je\n",
            "\tTarget:  puisje louer des raquettes\n",
            "Epoch: 24  loss: 68.27574068307877\n",
            "\tOutput:  <eos> <eos> <eos> <eos> <eos>\n",
            "\tTarget:  tu es un bon gamin\n",
            "Epoch: 25  loss: 67.97537419199944\n",
            "\tOutput:  <eos> <eos> <eos>\n",
            "\tTarget:  pourquoi r sistestu\n",
            "Epoch: 26  loss: 67.77727884054184\n",
            "\tOutput:  <eos> <eos> <eos> <eos> <eos>\n",
            "\tTarget:  de qui tesvous le fils\n",
            "Epoch: 27  loss: 67.34574630856514\n",
            "\tOutput:  <eos> <eos> <eos> <eos>\n",
            "\tTarget:  tout le monde pleure\n",
            "Epoch: 28  loss: 67.30652239918709\n",
            "\tOutput:  <eos> <eos> <eos>\n",
            "\tTarget:  bois ton lait\n",
            "Epoch: 29  loss: 66.97130754590034\n",
            "\tOutput:  <eos> <eos> <eos>\n",
            "\tTarget:  pourquoi tesvous seules\n",
            "Epoch: 30  loss: 66.7815414071083\n",
            "\tOutput:  <eos> <eos> <eos> <eos> <eos>\n",
            "\tTarget:  vous tes venus trop tard\n",
            "Epoch: 31  loss: 66.4888471364975\n",
            "\tOutput:  <eos> <eos>\n",
            "\tTarget:  demandelui conseil\n",
            "Epoch: 32  loss: 66.30434030294418\n",
            "\tOutput:  <eos> <eos> <eos> <eos> <eos>\n",
            "\tTarget:  tout le monde est super\n",
            "Epoch: 33  loss: 66.00808125734329\n",
            "\tOutput:  vous vous tes tes tes\n",
            "\tTarget:  vous tes tr s sages\n",
            "Epoch: 34  loss: 65.76619505882263\n",
            "\tOutput:  vous vous vous vous vous\n",
            "\tTarget:  tu es tr s vive\n",
            "Epoch: 35  loss: 65.61019110679626\n",
            "\tOutput:  <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "\tTarget:  astu vu qui que ce soit\n",
            "Epoch: 36  loss: 65.47030085325241\n",
            "\tOutput:  c'est c'est c'est c'est c'est\n",
            "\tTarget:  qui a fait cette tourte\n",
            "Epoch: 37  loss: 65.26487773656845\n",
            "\tOutput:  <eos> <eos> <eos> <eos>\n",
            "\tTarget:  pourquoi tout ce drame\n",
            "Epoch: 38  loss: 65.01492875814438\n",
            "\tOutput:  je je je\n",
            "\tTarget:  aije le choix\n",
            "Epoch: 39  loss: 64.85464525222778\n",
            "\tOutput:  c'est c'est c'est c'est\n",
            "\tTarget:  avezvous v rifi ceci\n",
            "Epoch: 40  loss: 64.63245850801468\n",
            "\tOutput:  tu vous vous\n",
            "\tTarget:  n'astu pas soif\n",
            "Epoch: 41  loss: 64.52160203456879\n",
            "\tOutput:  <eos> <eos> <eos>\n",
            "\tTarget:  tu devrais essayer\n",
            "Epoch: 42  loss: 64.29592603445053\n",
            "\tOutput:  <eos> <eos> <eos> <eos> <eos>\n",
            "\tTarget:  sur place ou pour emporter\n",
            "Epoch: 43  loss: 64.11325666308403\n",
            "\tOutput:  elle elle <eos> <eos>\n",
            "\tTarget:  vous avez raison tom\n",
            "Epoch: 44  loss: 63.94493183493614\n",
            "\tOutput:  vous vous tes tes\n",
            "\tTarget:  vous tes trop maigrichon\n",
            "Epoch: 45  loss: 63.905257403850555\n",
            "\tOutput:  vous vous tes tes tes\n",
            "\tTarget:  tu es tr s peureux\n",
            "Epoch: 46  loss: 63.69277465343475\n",
            "\tOutput:  est <eos> <eos> <eos> <eos>\n",
            "\tTarget:  tout le monde est super\n",
            "Epoch: 47  loss: 63.52037641406059\n",
            "\tOutput:  je je <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
            "\tTarget:  tu ne peux pas simplement t'en aller comme a\n",
            "Epoch: 48  loss: 63.339835703372955\n",
            "\tOutput:  vous vous <eos> <eos>\n",
            "\tTarget:  vous tes trop maigrichonnes\n",
            "Epoch: 49  loss: 63.25271102786064\n",
            "\tOutput:  ne ne ne ne\n",
            "\tTarget:  n'essaie pas trop fort\n",
            "Epoch: 50  loss: 63.06438344717026\n",
            "\tOutput:  vous vous <eos> <eos> <eos>\n",
            "\tTarget:  tu es un bon gamin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLg0PpWO9VTn",
        "colab_type": "text"
      },
      "source": [
        "# Test your translator\n",
        "\n",
        "##### Unless you speak french you're going have to check it with google translate https://translate.google.com/\n",
        "##### I found it started doing alright once the loss got below 10 but this might take a while"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEVCcaKu9VTo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8bd3aa46-ba9a-42aa-8bc2-1bd1cda6b1e7"
      },
      "source": [
        "sentence = 'how are you'\n",
        "translate(sentence)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'comment vastu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNXvvca89VTw",
        "colab_type": "text"
      },
      "source": [
        "#### Test it on testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxvHJSyR9VTx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "33c8798c-f6a1-4a29-dda1-63667dc0d463"
      },
      "source": [
        "model.eval()\n",
        "scores = []\n",
        "preds = []\n",
        "targetlist = []\n",
        "for j,(context, target) in enumerate(testloader):\n",
        "        trg_input = target[:,:-1]\n",
        "        targets = target[:,1:].contiguous().view(-1)\n",
        "        targetlist.append(targets)\n",
        "        src,trg = mask(context,trg_input)\n",
        "        output = model(context,trg_input,src,trg)\n",
        "        pred = F.softmax(output,2).argmax(2)\n",
        "        preds.append(pred)\n",
        "        correct = sum(pred[0][targets!=fr_to_ix['<pad>']]==targets[targets!=fr_to_ix['<pad>']]).item()/len(targets[targets!=fr_to_ix['<pad>']])\n",
        "        scores.append(correct)\n",
        "plt.plot(scores)\n",
        "print('Average # of words correct',np.mean(scores))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average # of words correct 0.6032565295815295\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H28_e8NE9VT2",
        "colab_type": "text"
      },
      "source": [
        "# MULTI-HEAD ATTENTION (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E69Y6dea9VT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class multi_attention(nn.Module):\n",
        "    def __init__(self,dim,encoder_dim,dropout = .1):\n",
        "        super().__init__()\n",
        "        \n",
        "        heads = []\n",
        "        for i in range(5): ### TODO ### CHOOSE THE # OF HEADS YOU WANT\n",
        "            heads.append(self_attention(dim,encoder_dim)) ### TODO ### ADD SELF_ATTENTION LAYERS TO HEADS\n",
        "        \n",
        "        self.heads = nn.ModuleList(heads)\n",
        "        \n",
        "        self.linear = nn.Linear(len(heads)*encoder_dim,encoder_dim) ### TODO ###\n",
        "    \n",
        "    \n",
        "    def forward(self,x,mask=None):\n",
        "        headoutputs = [layer(x,mask) for layer in self.heads]\n",
        "        headoutputs = torch.cat(headoutputs,dim=2)\n",
        "        return self.linear(headoutputs)\n",
        "    \n",
        "class encoder(nn.Module):\n",
        "    def __init__(self,dim,enc_dim,vocab_size,dropout=.1):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.enc_dim = enc_dim\n",
        "        self.attention = multi_attention(dim,enc_dim,dropout)\n",
        "        self.norm1 = nn.LayerNorm(enc_dim)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(enc_dim,enc_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.residual = nn.Linear(dim,enc_dim)\n",
        "        self.norm2 = nn.LayerNorm(enc_dim)\n",
        "    \n",
        "    def forward(self,x,mask):\n",
        "        z = self.attention(x,mask)\n",
        "        if self.dim != self.enc_dim:\n",
        "            x = self.residual(x)\n",
        "        z = self.norm1(x+z)\n",
        "        z2 = self.linear(z)\n",
        "        return self.norm2(z+z2)\n",
        "     \n",
        "    \n",
        "class decoder(nn.Module):\n",
        "    def __init__(self,dim,input_size,vocab_size,dropout=.1):\n",
        "        super().__init__()\n",
        "        self.attention = multi_attention(input_size,dim,dropout)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.EDattention = encdec_attention(dim,dropout)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(dim,dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.norm3 = nn.LayerNorm(dim)\n",
        "    \n",
        "    def forward(self,x,k,v,src,trg):\n",
        "        z = self.attention(x,trg)\n",
        "        z = self.norm1(z+x)\n",
        "        z2 = self.EDattention(z,k,v,src)\n",
        "        z2 = self.norm2(z2+z)\n",
        "        z3 = self.linear(z2)\n",
        "        return self.norm3(z3+z2)\n",
        "    \n",
        "class transformer(nn.Module):\n",
        "    def __init__(self,dim,encoder_dim,enc_vocab_size,dec_vocab_size,input_size):\n",
        "        super().__init__()\n",
        "        self.embedding1 = nn.Embedding(enc_vocab_size,dim)\n",
        "        self.embedding2 = nn.Embedding(dec_vocab_size,encoder_dim)\n",
        "        \n",
        "        self.pe1 = PositionalEncoder(dim,enmax)\n",
        "        self.pe2 = PositionalEncoder(encoder_dim,frmax)\n",
        "        self.encoders = []\n",
        "    \n",
        "        self.encoders.append(encoder(dim,encoder_dim,enc_vocab_size))   ### FEEL FREE TO ADD OR REMOVE ENCODERS\n",
        "        self.encoders = nn.ModuleList(self.encoders)\n",
        "        \n",
        "        self.decoders = []\n",
        "        self.decoders.append(decoder(encoder_dim,encoder_dim,dec_vocab_size)) ### FEEL FREE TO ADD OR REMOVE ENCODERS\n",
        "        self.decoders = nn.ModuleList(self.decoders)\n",
        "        \n",
        "        self.final = nn.Sequential(\n",
        "            nn.Linear(encoder_dim,dec_vocab_size),\n",
        "            nn.LogSoftmax(2)\n",
        "        )\n",
        "        \n",
        "        self.k = nn.Linear(encoder_dim,encoder_dim)\n",
        "        self.v = nn.Linear(encoder_dim,encoder_dim)\n",
        "        \n",
        "    def create_dec_KV(self,z):\n",
        "        K = self.k(z)\n",
        "        V = self.v(z)\n",
        "        return K,V\n",
        "    \n",
        "    def encode(self,x,src):\n",
        "        x = self.embedding1(x)\n",
        "        x = self.pe1(x)\n",
        "        for layer in self.encoders:\n",
        "            x = layer(x,src)\n",
        "        return x\n",
        "    \n",
        "    def decode(self,y,K,V,src,trg):\n",
        "        y = self.embedding2(y)\n",
        "        y = self.pe2(y)\n",
        "        for layer in self.decoders:\n",
        "            y = layer(y,K,V,src,trg)\n",
        "        return self.final(y)\n",
        "        \n",
        "    \n",
        "    def forward(self,x,y,src,trg):\n",
        "        x = self.encode(x,src)\n",
        "        K,V = self.create_dec_KV(x)\n",
        "        y = self.decode(y,K,V,src,trg)\n",
        "        \n",
        "        return y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95I0NrZT9VT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = transformer(enmax,enmax,len(envocab),len(frvocab),frmax)\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2uJKA6h9VUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.01,weight_decay=.00001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=.1,patience=10,threshold=1,min_lr=.0001)\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Zx-tNZeQ9VUG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ff755001-a1ba-4aac-ab2d-36c54ccaa459"
      },
      "source": [
        "for i in range(50):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for j,(context, target) in enumerate(trainloader):\n",
        "        trg_input = target[:,:-1]\n",
        "        outmask = target[:,1:]!= fr_to_ix['<pad>']\n",
        "        targets = target[:,1:].contiguous().view(-1)\n",
        "        src,trg = mask(context,trg_input)\n",
        "        output = model(context,trg_input,src,trg)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output.view(output.shape[0]*output.shape[1],-1),targets)\n",
        "        loss.backward()\n",
        "        total_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    scheduler.step(total_loss)\n",
        "    print('Epoch:', i+1,' loss:', total_loss)\n",
        "    model.eval()\n",
        "    scores = []\n",
        "    preds = []\n",
        "    targetlist = []\n",
        "    for j,(context, target) in enumerate(valloader):\n",
        "            trg_input = target[:,:-1]\n",
        "            targets = target.contiguous().view(-1)\n",
        "            targetlist.append(targets)\n",
        "            src,trg = mask(context,trg_input)\n",
        "            output = model(context,trg_input,src,trg)\n",
        "            pred = F.softmax(output,2).argmax(2)\n",
        "            preds.append(pred)\n",
        "            break\n",
        "    compareoutput(preds,targetlist,loc=0)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  loss: 133.51733791828156\n",
            "\tOutput:  je <eos> <eos>\n",
            "\tTarget:  comptez jusqu' trente\n",
            "Epoch: 2  loss: 60.46332961320877\n",
            "\tOutput:  c'est <eos> voiture\n",
            "\tTarget:  choisissez votre arme\n",
            "Epoch: 3  loss: 47.60376292467117\n",
            "\tOutput:  vous me voir\n",
            "\tTarget:  peuxtu me pardonner\n",
            "Epoch: 4  loss: 40.62639278173447\n",
            "\tOutput:  tu tes en\n",
            "\tTarget:  vous tes surmen\n",
            "Epoch: 5  loss: 35.81241875886917\n",
            "\tOutput:  tu es <eos> <eos>\n",
            "\tTarget:  tu me fais peur\n",
            "Epoch: 6  loss: 32.113792687654495\n",
            "\tOutput:  vous tes tr s <eos>\n",
            "\tTarget:  vous tes tr s courageux\n",
            "Epoch: 7  loss: 29.19096028804779\n",
            "\tOutput:  tu une tu veux <eos> hicule\n",
            "\tTarget:  voulezvous que je vous v hicule\n",
            "Epoch: 8  loss: 27.022227451205254\n",
            "\tOutput:  vous tes tr s ch col re\n",
            "\tTarget:  vous tes tr s en col re\n",
            "Epoch: 9  loss: 25.40619035065174\n",
            "\tOutput:  ne faites pas trop\n",
            "\tTarget:  n'en dites pas trop\n",
            "Epoch: 10  loss: 24.217305794358253\n",
            "\tOutput:  peuxtu peux le aller <eos>\n",
            "\tTarget:  tu peux te garer ici\n",
            "Epoch: 11  loss: 23.12133425474167\n",
            "\tOutput:  puisje me la v\n",
            "\tTarget:  puisje emprunter le tien\n",
            "Epoch: 12  loss: 16.819139949977398\n",
            "\tOutput:  vous le ais\n",
            "\tTarget:  parlestu fran ais\n",
            "Epoch: 13  loss: 14.36696208268404\n",
            "\tOutput:  tout le monde tait t sur\n",
            "\tTarget:  tout le monde a t vigilant\n",
            "Epoch: 14  loss: 13.292954877018929\n",
            "\tOutput:  vous tes beau\n",
            "\tTarget:  vous tes belles\n",
            "Epoch: 15  loss: 12.510266840457916\n",
            "\tOutput:  vous tes insaisissable\n",
            "\tTarget:  vous tes productif\n",
            "Epoch: 16  loss: 12.012225475162268\n",
            "\tOutput:  tout le monde rit en train de lire\n",
            "\tTarget:  tout le monde est en train de lire\n",
            "Epoch: 17  loss: 11.473853778094053\n",
            "\tOutput:  les p <eos> plein\n",
            "\tTarget:  ton pneu est crev\n",
            "Epoch: 18  loss: 11.080219380557537\n",
            "\tOutput:  tesvous la\n",
            "\tTarget:  aimestu boston\n",
            "Epoch: 19  loss: 10.79738860949874\n",
            "\tOutput:  facile est facile <eos> <eos> <eos> facile\n",
            "\tTarget:  aussit t gagn aussit t d pens\n",
            "Epoch: 20  loss: 10.473023973405361\n",
            "\tOutput:  a a\n",
            "\tTarget:  tout importe\n",
            "Epoch: 21  loss: 10.169070240110159\n",
            "\tOutput:  puisje ton votre stylo\n",
            "\tTarget:  puisje utiliser ton stylo\n",
            "Epoch: 22  loss: 9.940905030816793\n",
            "\tOutput:  vous vu <eos> que ce soit\n",
            "\tTarget:  avezvous vu qui que ce soit\n",
            "Epoch: 23  loss: 8.881636034697294\n",
            "\tOutput:  pourquoi estu\n",
            "\tTarget:  pourquoi menstu\n",
            "Epoch: 24  loss: 8.73407106474042\n",
            "\tOutput:  tu semblez mieux d'y aller\n",
            "\tTarget:  vous feriez mieux d'y aller\n",
            "Epoch: 25  loss: 8.608581572771072\n",
            "\tOutput:  pourquoi estu\n",
            "\tTarget:  pourquoi menstu\n",
            "Epoch: 26  loss: 8.577987808734179\n",
            "\tOutput:  tesvous en tre\n",
            "\tTarget:  estu en vacances\n",
            "Epoch: 27  loss: 8.529230646789074\n",
            "\tOutput:  qui t'a envoy ici\n",
            "\tTarget:  qui t'a envoy ici\n",
            "Epoch: 28  loss: 8.543880604207516\n",
            "\tOutput:  un enfant <eos>\n",
            "\tTarget:  un enfant manque\n",
            "Epoch: 29  loss: 8.4617888815701\n",
            "\tOutput:  je une\n",
            "\tTarget:  revenez demain\n",
            "Epoch: 30  loss: 8.401316720992327\n",
            "\tOutput:  tout le monde le <eos>\n",
            "\tTarget:  tout le monde t'aime bien\n",
            "Epoch: 31  loss: 8.311887837946415\n",
            "\tOutput:  vous me montrer\n",
            "\tTarget:  pouvezvous me pardonner\n",
            "Epoch: 32  loss: 8.287710089236498\n",
            "\tOutput:  votre temps est coul\n",
            "\tTarget:  ton temps est coul\n",
            "Epoch: 33  loss: 8.280831374228\n",
            "\tOutput:  vous peux les les deux\n",
            "\tTarget:  tu peux avoir les deux\n",
            "Epoch: 34  loss: 8.30528461188078\n",
            "\tOutput:  ma ma c'est vie\n",
            "\tTarget:  r ponds ma question\n",
            "Epoch: 35  loss: 8.183666203171015\n",
            "\tOutput:  tesvous tesvous s re\n",
            "\tTarget:  en estu s re\n",
            "Epoch: 36  loss: 8.205848902463913\n",
            "\tOutput:  signe <eos> lumi\n",
            "\tTarget:  rappellemoi de suite\n",
            "Epoch: 37  loss: 8.147554278373718\n",
            "\tOutput:  j'ai besoin de raison\n",
            "\tTarget:  aije besoin d'une raison\n",
            "Epoch: 38  loss: 8.086129494011402\n",
            "\tOutput:  vous tes beau\n",
            "\tTarget:  vous tes beau\n",
            "Epoch: 39  loss: 8.069836027920246\n",
            "\tOutput:  tesvous dyslexique\n",
            "\tTarget:  tesvous productive\n",
            "Epoch: 40  loss: 8.006640944629908\n",
            "\tOutput:  vous le ais\n",
            "\tTarget:  parlestu fran ais\n",
            "Epoch: 41  loss: 7.967869363725185\n",
            "\tOutput:  ne pr pas <eos> risques\n",
            "\tTarget:  ne jetez pas de pierres\n",
            "Epoch: 42  loss: 7.966028939932585\n",
            "\tOutput:  vous une vous vous <eos>\n",
            "\tTarget:  veuxtu que je t'emm ne\n",
            "Epoch: 43  loss: 7.910676293075085\n",
            "\tOutput:  tu devais mieux d'y vous\n",
            "\tTarget:  tu ferais mieux de partir\n",
            "Epoch: 44  loss: 7.8820197358727455\n",
            "\tOutput:  puisje parler parler\n",
            "\tTarget:  puisje te parler\n",
            "Epoch: 45  loss: 7.878735199570656\n",
            "\tOutput:  tu tes insensible\n",
            "\tTarget:  vous tes productifs\n",
            "Epoch: 46  loss: 7.900113075971603\n",
            "\tOutput:  vous es pas gar\n",
            "\tTarget:  tu n'es qu'un enfant\n",
            "Epoch: 47  loss: 7.785122524946928\n",
            "\tOutput:  veuxtu <eos> <eos>\n",
            "\tTarget:  vastu t'en servir\n",
            "Epoch: 48  loss: 7.718424692749977\n",
            "\tOutput:  c'est a bon bon\n",
            "\tTarget:  waou c'est pas cher\n",
            "Epoch: 49  loss: 7.713570799678564\n",
            "\tOutput:  tu ne peux pas vous vous\n",
            "\tTarget:  tu ne peux pas le manquer\n",
            "Epoch: 50  loss: 7.721912868320942\n",
            "\tOutput:  prenez tom <eos>\n",
            "\tTarget:  trouvez quelqu'un d'autre\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e4zdQbt9VUK",
        "colab_type": "text"
      },
      "source": [
        "# Test your  multi-head transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoP5X1gw9VUL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "46525fd2-985f-4df9-f959-099248caa030"
      },
      "source": [
        "sentence = 'how are you'\n",
        "translate(sentence)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'comment vastu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wde119EL9VUP",
        "colab_type": "text"
      },
      "source": [
        "# QUESTION\n",
        "\n",
        "#### 1) Was the runtime of your multi-head transformer noticably longer than the single head one? What about the speed the loss decreased? If you had the time and resources to train it to a good spot, how did the translation quality compare to the single-headed transformer?\n",
        "\n",
        "ANSWER: I did't notice the runtime of the multi-head tranformer was longer. It is possible I didn't implement it correctly. It appears that the loss started a bit higher but was able to get down to where the single-head transformer was and accurately translated the sentence.\n",
        "\n",
        "#### 2)Try adding encoders and decoders to one of your transformers. Does having the extra layers improve performance? How does it affect runtime?\n",
        "\n",
        "Having extra layers (3) made the runtime much longer and the performance worse. Overall, the multi-head did better than having extra encoder/decoder layers. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-4rBec69VUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}